{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5zPU4pHdEHf"
      },
      "source": [
        "## 1. CNN (with GloVe Embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MPH-DK7t3PpF"
      },
      "outputs": [],
      "source": [
        "#Dataframe\n",
        "import pandas as pd\n",
        "\n",
        "#Matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "#Scki-kit learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "#Preprocessing\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "#Tensor flow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.initializers import Constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk as nltk\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdIRpfIE60F4",
        "outputId": "9ee752d7-93cb-4641-b141-d16b2c70de40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/katarinalitricin/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/katarinalitricin/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/katarinalitricin/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M549GC-m1SD",
        "outputId": "b028596b-6cf9-4339-8610-3e61dc596c61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading builder script: 100%|██████████| 4.03k/4.03k [00:00<00:00, 7.34MB/s]\n",
            "Downloading readme: 100%|██████████| 6.84k/6.84k [00:00<00:00, 9.60MB/s]\n",
            "Downloading data: 100%|██████████| 81.4M/81.4M [00:14<00:00, 5.58MB/s]  \n",
            "Generating train split: 100%|██████████| 1600000/1600000 [00:38<00:00, 41370.58 examples/s]\n",
            "Generating test split: 100%|██████████| 498/498 [00:00<00:00, 36113.41 examples/s]\n"
          ]
        }
      ],
      "source": [
        "#loading the dataset\n",
        "dataset = load_dataset(\"sentiment140\", trust_remote_code = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IBnH3kO5uR03"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(dataset['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "JurPMDF15XV9",
        "outputId": "ebd4b366-6558-466f-c489-660c3f07845a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>date</th>\n",
              "      <th>user</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>query</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>0</td>\n",
              "      <td>NO_QUERY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>0</td>\n",
              "      <td>NO_QUERY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>0</td>\n",
              "      <td>NO_QUERY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>0</td>\n",
              "      <td>NO_QUERY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>0</td>\n",
              "      <td>NO_QUERY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>AmandaMarie1028</td>\n",
              "      <td>4</td>\n",
              "      <td>NO_QUERY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>TheWDBoards</td>\n",
              "      <td>4</td>\n",
              "      <td>NO_QUERY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>bpbabe</td>\n",
              "      <td>4</td>\n",
              "      <td>NO_QUERY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>tinydiamondz</td>\n",
              "      <td>4</td>\n",
              "      <td>NO_QUERY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
              "      <td>RyanTrevMorris</td>\n",
              "      <td>4</td>\n",
              "      <td>NO_QUERY</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      text  \\\n",
              "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
              "1        is upset that he can't update his Facebook by ...   \n",
              "2        @Kenichan I dived many times for the ball. Man...   \n",
              "3          my whole body feels itchy and like its on fire    \n",
              "4        @nationwideclass no, it's not behaving at all....   \n",
              "...                                                    ...   \n",
              "1599995  Just woke up. Having no school is the best fee...   \n",
              "1599996  TheWDB.com - Very cool to hear old Walt interv...   \n",
              "1599997  Are you ready for your MoJo Makeover? Ask me f...   \n",
              "1599998  Happy 38th Birthday to my boo of alll time!!! ...   \n",
              "1599999  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
              "\n",
              "                                 date             user  sentiment     query  \n",
              "0        Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n",
              "1        Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n",
              "2        Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n",
              "3        Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n",
              "4        Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  \n",
              "...                               ...              ...        ...       ...  \n",
              "1599995  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028          4  NO_QUERY  \n",
              "1599996  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards          4  NO_QUERY  \n",
              "1599997  Tue Jun 16 08:40:49 PDT 2009           bpbabe          4  NO_QUERY  \n",
              "1599998  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz          4  NO_QUERY  \n",
              "1599999  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris          4  NO_QUERY  \n",
              "\n",
              "[1600000 rows x 5 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0K4sur9jvB2w"
      },
      "outputs": [],
      "source": [
        "# Rename 'sentiment' to 'target'\n",
        "df = df.rename(columns={\"sentiment\": \"target\"})\n",
        "\n",
        "# Drop rows with NaN values in 'target' and 'text'\n",
        "df.dropna(subset=['target', 'text'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rzxb_kUy5GbU"
      },
      "outputs": [],
      "source": [
        "# convert to binary classification\n",
        "df['target'] = df['target'].apply(lambda x: 1 if x == 4 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ulGs8cMd5JSm"
      },
      "outputs": [],
      "source": [
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase and remove tags\n",
        "    text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df[\"text\"] = df[\"text\"].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZiR9Wn4H554e"
      },
      "outputs": [],
      "source": [
        "max_words = 10000\n",
        "max_len = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GbtzQr4E6L-Q"
      },
      "outputs": [],
      "source": [
        "#Splitting the data into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "u7UbLYHt59NK"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atoWKRZgtW1s"
      },
      "source": [
        "### GloVe embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download and extraction complete.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Download the GloVe embeddings\n",
        "url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "file_name = \"glove.6B.zip\"\n",
        "\n",
        "# Download the file\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "# Optionally, remove the zip file to clean up\n",
        "os.remove(file_name)\n",
        "\n",
        "print(\"Download and extraction complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGFNp8CiSfVX"
      },
      "source": [
        " Load GloVe Embeddings into a Matrix:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbhp8rTCb_sf"
      },
      "source": [
        "Pre-trained GloVe embeddings capture semantic relationships between words based on vast amounts of text data, providing a richer starting point for the model compared to random initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ut5_XPN8w36U"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "embedding_index = {}\n",
        "\n",
        "# Make sure the 'glove.6B.100d.txt' file is in your working directory\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ttZAE_yDWT-h"
      },
      "outputs": [],
      "source": [
        "max_len = 150  # Increased sequence length!!\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvZHVz6ET5y_",
        "outputId": "52bea500-4ff3-45a9-8aaf-d420ef10c3c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 150, 100)          1000000   \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 146, 128)          64128     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 29, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 29, 128)           0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 25, 128)           82048     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 5, 128)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 5, 128)            0         \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 1, 128)            82048     \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 128)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1244865 (4.75 MB)\n",
            "Trainable params: 244865 (956.50 KB)\n",
            "Non-trainable params: 1000000 (3.81 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "model1 = Sequential([\n",
        "    Embedding(max_words, embedding_dim, embeddings_initializer=Constant(embedding_matrix), input_length=max_len, trainable=False),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=5),\n",
        "    Dropout(0.2),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=5),\n",
        "    Dropout(0.2),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model1.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21HJNKlbtLZM"
      },
      "source": [
        "Training the cnn model with GloVe embeddings using 10 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "AstdKY-vwz-P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "32000/32000 [==============================] - 804s 25ms/step - loss: 0.5721 - accuracy: 0.6995 - val_loss: 0.5491 - val_accuracy: 0.7158 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "32000/32000 [==============================] - 756s 24ms/step - loss: 0.5546 - accuracy: 0.7142 - val_loss: 0.5416 - val_accuracy: 0.7203 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "32000/32000 [==============================] - 787s 25ms/step - loss: 0.5485 - accuracy: 0.7195 - val_loss: 0.5466 - val_accuracy: 0.7190 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "32000/32000 [==============================] - 790s 25ms/step - loss: 0.5437 - accuracy: 0.7229 - val_loss: 0.5407 - val_accuracy: 0.7244 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "32000/32000 [==============================] - 773s 24ms/step - loss: 0.5403 - accuracy: 0.7256 - val_loss: 0.5354 - val_accuracy: 0.7255 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "32000/32000 [==============================] - 784s 24ms/step - loss: 0.5382 - accuracy: 0.7274 - val_loss: 0.5382 - val_accuracy: 0.7240 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "32000/32000 [==============================] - 789s 25ms/step - loss: 0.5363 - accuracy: 0.7286 - val_loss: 0.5340 - val_accuracy: 0.7259 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "32000/32000 [==============================] - 772s 24ms/step - loss: 0.5349 - accuracy: 0.7303 - val_loss: 0.5375 - val_accuracy: 0.7260 - lr: 0.0010\n",
            "Epoch 9/10\n",
            "32000/32000 [==============================] - 748s 23ms/step - loss: 0.5335 - accuracy: 0.7311 - val_loss: 0.5351 - val_accuracy: 0.7262 - lr: 0.0010\n",
            "Epoch 10/10\n",
            "32000/32000 [==============================] - 768s 24ms/step - loss: 0.5220 - accuracy: 0.7379 - val_loss: 0.5308 - val_accuracy: 0.7295 - lr: 2.0000e-04\n"
          ]
        }
      ],
      "source": [
        "# Callbacks for Early Stopping and Learning Rate Reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "\n",
        "# Continue training for more epochs\n",
        "history = model1.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MA5l_EtfYXkr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 75s 7ms/step - loss: 0.5323 - accuracy: 0.7290\n",
            "Final Test Accuracy: 0.7290499806404114\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model after extended training\n",
        "final_loss, final_accuracy = model1.evaluate(X_test_pad, y_test)\n",
        "print(f'Final Test Accuracy: {final_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model architecture:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Embedding Layer\n",
        "\n",
        "The embedding layer transforms input words into dense vector representations using pre-trained GloVe embeddings. GloVe (Global Vectors for Word Representation) embeddings are pre-trained on large corpora and capture semantic relationships between words. Using these embeddings helps our model leverage this rich, pre-existing knowledge, improving its performance even with limited training data. The layer takes sequences of integers (each representing a word) as input and produces dense vector representations for each word in the sequence.\n",
        "\n",
        "- Convolutional Layers\n",
        "\n",
        "The convolutional layers apply convolution operations to capture local patterns and features in the text data. CNNs are effective at capturing local dependencies and n-gram features in text, making them suitable for text classification tasks. The choice of filters and kernel size is crucial:\n",
        "\n",
        "-- Number of Filters (128): A larger number of filters can capture more diverse features. The choice of 128 filters balances computational efficiency with the ability to learn rich features.\n",
        "\n",
        "-- Kernel Size (5): A kernel size of 5 allows the model to consider 5-gram sequences at a time, which helps in capturing meaningful patterns that span multiple words.\n",
        "\n",
        "- Max Pooling Layers\n",
        "\n",
        "Pooling layers reduce the dimensionality of the feature maps by taking the maximum value over a specified window. This helps in reducing the computation cost and controlling overfitting by providing a form of down-sampling. A pooling size of 2 is used to reduce the feature map size by half, which helps in reducing the computational load and retaining the most important features.\n",
        "\n",
        "-- Global Max Pooling Layer\n",
        "\n",
        "A global max pooling layer reduces each feature map to a single value by taking the maximum value. This is useful to get a fixed-length output regardless of the input size. It further downsamples the feature maps to a fixed-length vector, which is essential for feeding into dense layers.\n",
        "\n",
        "- Dense Layers\n",
        "\n",
        "Dense (fully connected) layers take the features extracted by the convolutional and pooling layers and perform the actual classification. Dense layers are used to combine the features learned by previous layers and make the final classification decision. We use a moderate number of units (128) to balance complexity and performance, and the ReLU (Rectified Linear Unit) activation function introduces non-linearity, enabling the model to learn more complex patterns.\n",
        "\n",
        "- Dropout Layer\n",
        "\n",
        "The dropout layer prevents overfitting by randomly setting a fraction of input units to 0 during training. Dropout is a regularization technique that helps prevent overfitting by ensuring the model does not rely too heavily on any single feature. A dropout rate of 0.5 is a common choice that balances regularization and performance.\n",
        "\n",
        "- Output Layer\n",
        "\n",
        "The output layer produces the final binary classification output. The sigmoid activation function is used, as it outputs a probability value between 0 and 1, which is suitable for binary classification tasks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
